{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os, shutil\n",
    "import glob\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\n",
    "import pickle\n",
    "import random\n",
    "import zlib\n",
    "random.seed = 0\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import keras.backend as k\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import l2\n",
    "from clarifai.rest import ClarifaiApp\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shutil\n",
    "import Algorithmia\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ditto:\n",
    "    def __init__(self, folder, num_api_calls, num_classes, api, test_size=.1,  img_or_nlp='img'):\n",
    "        self.folder = folder+'/'\n",
    "        self.api = api\n",
    "        self.num_api_calls = num_api_calls\n",
    "        self.num_classes = num_classes\n",
    "        self.all_generated_filepaths = []\n",
    "        self.newest_file_paths = []\n",
    "        self.predictions = []\n",
    "        self.analysis_type = str(img_or_nlp)\n",
    "        self.calls_to_api = 0\n",
    "        self.test_size=test_size\n",
    "        self.gradients =[]\n",
    "    \n",
    "    def generate_cnn_dataset(self, min_generations=3):\n",
    "        self.dir_path = self.folder+'Ditto'\n",
    "        if os.path.exists(self.dir_path):\n",
    "            shutil.rmtree(self.dir_path)\n",
    "        os.mkdir(self.dir_path)\n",
    "        self.test_dir_path = self.dir_path+'/Test'\n",
    "        os.mkdir(self.test_dir_path)\n",
    "        gens_vs_sample_size = {}\n",
    "        num_test = int((self.num_api_calls*self.test_size)//1)\n",
    "        max_gens = np.log(self.num_api_calls-num_test)/np.log(self.num_classes)//1\n",
    "        for i in range(min_generations,int(max_gens)):\n",
    "            gens_vs_sample_size.update({i:(self.num_api_calls/self.num_classes**(i))//1})\n",
    "        print(gens_vs_sample_size)\n",
    "        gens = int(input('How many generations? '))\n",
    "        self.init_gens = gens\n",
    "        self.gens_to_go = gens\n",
    "        init_sample_size = gens_vs_sample_size[gens]\n",
    "        self.init_sample_size = int(init_sample_size)\n",
    "        \n",
    "        pics = [file for file in os.listdir(str(self.folder))]\n",
    "        sample = random.sample(pics,k=self.init_sample_size)\n",
    "        for pic in sample:\n",
    "            filepath = self.folder+pic\n",
    "            shutil.copy(filepath,self.dir_path)\n",
    "        unused_pics = [file for file in pics if file not in sample]\n",
    "        test_set = random.sample(unused_pics,k=num_test)\n",
    "        unused_pics = [file for file in pics if (file not in sample)&(file not in test_set)]\n",
    "        self.unused_pics = unused_pics\n",
    "        self.test_set_paths = []\n",
    "        for pic in test_set:\n",
    "            filepath = self.folder+'/'+pic\n",
    "            shutil.copy(filepath,self.test_dir_path)\n",
    "            self.test_set_paths.append(filepath)\n",
    "        self.target = [int(file.split('_')[1]) for file in sample]\n",
    "        self.target = to_categorical(self.target)\n",
    "        self.val_target = [int(file.split('_')[1]) for file in test_set]\n",
    "        self.val_target = to_categorical(self.val_target)\n",
    "        dataset = np.ndarray(shape=(self.init_sample_size, 224, 224, 3))\n",
    "        for i in range(self.init_sample_size):\n",
    "            img = load_img(self.folder+'/'+sample[i],target_size = (224,224))\n",
    "            img_arr = img_to_array(img)\n",
    "            arr = np.expand_dims(img_arr, axis=0)\n",
    "            arr = preprocess_input(arr)\n",
    "            dataset[i]=arr\n",
    "        self.cnn_dataset = dataset\n",
    "        self.init_files = sample \n",
    "        self.init_pics_paths = [self.folder+'/'+i for i in sample]\n",
    "        test_dataset = np.ndarray(shape=(len(self.test_set_paths), 224, 224, 3))\n",
    "        for i in range(len(self.test_set_paths)):\n",
    "            img = load_img(self.test_set_paths[i],target_size = (224,224))\n",
    "            img_arr = img_to_array(img)\n",
    "            arr = np.expand_dims(img_arr, axis=0)\n",
    "            arr = preprocess_input(arr)\n",
    "            test_dataset[i]=arr\n",
    "        self.test_dataset = test_dataset\n",
    "        self.other_pics = unused_pics\n",
    "    \n",
    "    def instantiate_nn(self,optimizer='default',loss='default',metric='default',output_activation='default'):\n",
    "        if self.analysis_type=='img':\n",
    "            if optimizer=='default':\n",
    "                sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "                optimizer='sgd'\n",
    "            if loss=='default':\n",
    "                loss='categorical_crossentropy'\n",
    "            if metric == 'default':\n",
    "                metric = 'accuracy'\n",
    "            if output_activation=='default':\n",
    "                output_activation = 'softmax'\n",
    "            model = models.Sequential()\n",
    "            model.add(layers.InputLayer(input_shape=(224,224,3)))\n",
    "            model.add(layers.Dropout(.3))\n",
    "            model.add(layers.convolutional.Conv2D(10, (2,2), strides=(1, 1), kernel_regularizer=l2(.01), padding='same', activation='relu'))\n",
    "            model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(layers.Dropout(.5))\n",
    "            model.add(layers.convolutional.Conv2D(20, (3,3), strides=(1, 1), padding='valid', activation='relu'))\n",
    "            model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(layers.Flatten())\n",
    "            model.add(layers.Dense(units=64, activation='relu'))\n",
    "            model.add(layers.Dropout(.5))\n",
    "            model.add(layers.Dense(units=32, activation='relu'))\n",
    "            model.add(layers.Dropout(.5))\n",
    "            model.add(layers.Dense(units=self.num_classes, input_dim=50,activation=output_activation))\n",
    "            model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "        #elif:\n",
    "            #self.analysis_type=='nlp':\n",
    "            self.model = model\n",
    "        else:\n",
    "            print('DIY')\n",
    "                \n",
    "    def Algorithmia_query(self, api_key):\n",
    "        client = Algorithmia.client(api_key)\n",
    "        algo = client.algo('deeplearning/GenderClassification/2.0.0')\n",
    "        testing_responses = []\n",
    "        for path in test_set_paths:\n",
    "            testing_responses.append(algo.pipe(path).result)\n",
    "        pickle.dump(testing_responses,open('algorithmia_testing_responses','wb'))\n",
    "        self.calls_to_api += len(testing_responses)\n",
    "        gender = []\n",
    "        for i in range(len(testing_responses[0]['results'][0]['gender'])):\n",
    "            gender.append(algorithmia_test['results'][0]['gender'][i]['confidence'])\n",
    "            \n",
    "            \n",
    "    def Clarifai_query(self, api_key=api_key, dataset=None):\n",
    "#     Query Clarifai\n",
    "        print('Querying Oracle')\n",
    "        if dataset==None:\n",
    "            dataset=self.cnn_dataset\n",
    "        training_responses = []\n",
    "        testing_responses = []\n",
    "        replacement_responses = []\n",
    "        problems = []\n",
    "        app = ClarifaiApp(api_key=api_key)\n",
    "        clarifai = app.models.get('demographics')\n",
    "        \n",
    "        #test set oracle predictions\n",
    "        filepaths = self.test_set_paths\n",
    "        for path in filepaths:\n",
    "            test_response = clarifai.predict_by_filename(path)\n",
    "            testing_responses.append(testing_response)\n",
    "            self.calls_to_api += 1\n",
    "        extract_values(testing_responses,train_or_test='test')\n",
    "        deal_with_problems(problems,self.test_folder)\n",
    "        \n",
    "        \n",
    "        #train set oracle predictions\n",
    "        if self.gens_to_go==self.init_gens:\n",
    "            filepaths = self.init_pics_paths\n",
    "        else:\n",
    "            filepaths =self.newest_file_paths\n",
    "        for path in filepaths:\n",
    "            response = clarifai.predict_by_filename(path)\n",
    "            training_responses.append(testing_response)\n",
    "            self.calls_to_api += 1\n",
    "        extract_values(testing_responses)\n",
    "        deal_with_problems(problems, self.folder)\n",
    "        #get responses for replacement pictures\n",
    "        extract_values(replacement_responses)\n",
    "        self.gens_to_go -= 1\n",
    "        \n",
    "\n",
    "#         if self.init_gens == self.gens_to_go:\n",
    "#             print(accuracy_score(self.target,(clarifai_predictions.astype(int)[:,1])))\n",
    "        \n",
    "    \n",
    "    def extract_values(responses,train_or_test='train'):\n",
    "        for i in range(len(responses)):\n",
    "            try:\n",
    "                gender_base = responses[i]['outputs'][0]['data']['regions'][0]['data']['face']['gender_appearance']['concepts']\n",
    "                genders = []\n",
    "                for j in range(len(gender_base)):\n",
    "                    value = gender_base[j]['value']\n",
    "                    genders.append(value)\n",
    "                if train_or_test == 'train':\n",
    "                    self.predictions.append(genders)\n",
    "                else:\n",
    "                    self.test_predictions.append(genders)\n",
    "            except:\n",
    "                if train_or_test == 'train':\n",
    "                    problems.append(i)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "    \n",
    "    def deal_with_problems(problems, train_or_test='test'):\n",
    "        problems = sorted(problem, reverse=True)\n",
    "        for i in problems:\n",
    "            del filepaths[i]\n",
    "        self.cnn_dataset = np.delete(self.cnn_dataset,problems,axis=0)\n",
    "        \n",
    "        if self.init_gens==self.gens_to_go:\n",
    "            for i in problems:\n",
    "                del self.target[i]\n",
    "            max_replacements = self.num_api_calls - self.calls_to_api*self.num_classes**self.init_gens\n",
    "            num_replacements = min([max_replacements,len(problems)])\n",
    "            replacements = random.sample(self.unused_pics,k=num_replacements)\n",
    "            for pic in replacements:\n",
    "                filepath = self.folder+pic\n",
    "                shutil.copy(filepath,folder)\n",
    "                replacement_responses.append(testing_response)\n",
    "                self.calls_to_api += 1\n",
    "\n",
    "    \n",
    "    def train_nn(self,model='default', data='default', predictions='default', epochs=10):\n",
    "        if data == 'default':\n",
    "            data = self.cnn_dataset\n",
    "        if predictions == 'default':\n",
    "            predictions = self.predictions\n",
    "        if model=='default':\n",
    "            model=self.model\n",
    "        if self.gens_to_go == 0:\n",
    "                callbacks = ModelCheckpoint('weights.best.hdf5', \n",
    "                                monitor='val_acc', \n",
    "                                verbose=1, \n",
    "                                save_best_only=True, \n",
    "                                mode='max')\n",
    "                model.fit(\n",
    "                    data,np.array(self.predictions),\n",
    "                    epochs=10,  \n",
    "                    batch_size=256,\n",
    "                    validation_data=(data[:self.init_sample_size],np.array(predictions)[:self.init_sample_size]),\n",
    "                    callbacks=[callbacks])\n",
    "                print(self.calls_to_api)\n",
    "                return data, predictions\n",
    "            \n",
    "            #Train CNN on dataset with oracle predictions = y\n",
    "        if (len(predictions) > 100) & (len(predictions) < 1000):\n",
    "            model.fit(data,\n",
    "                      np.array(predictions),\n",
    "                      epochs=10,\n",
    "                      batch_size=50) \n",
    "\n",
    "        elif len(self.predictions) > 1000:\n",
    "            model.fit(data,\n",
    "                      np.array(predictions),\n",
    "                      epochs=10,\n",
    "                      batch_size=256)\n",
    "\n",
    "        else:\n",
    "            model.fit(data,\n",
    "                      np.array(predictions),\n",
    "                      epochs=10) \n",
    "    \n",
    "    \n",
    "    def jacobian_augmentation(self, dataset='default',predictions='default',model='default', lmbda=0.1):\n",
    "        grads = []\n",
    "        self.newest_file_paths = []\n",
    "        if dataset=='default':\n",
    "            dataset = self.cnn_dataset\n",
    "        if model=='default':\n",
    "            model = self.model\n",
    "        if predictions=='default':\n",
    "            predictions = self.predictions\n",
    "        for i in tqdm.tqdm(range(self.num_classes)):\n",
    "            gradient = k.gradients(model.output[:,i],model.input)[0]\n",
    "            self.gradients.append(gradient)\n",
    "            session = k.get_session()\n",
    "            session.run(tf.initialize_all_variables())\n",
    "            grads.append(session.run([tf.sign(gradient)], feed_dict={model.input: dataset})[0])\n",
    "        synth = np.vstack([dataset, dataset])\n",
    "        predictions_list = [int(i[1]) for i in predictions]\n",
    "        n_preds = len(dataset)\n",
    "        for ind, x in tqdm.tqdm(enumerate(dataset)):\n",
    "            grad = grads[predictions_list[ind]][ind]\n",
    "            synth[n_preds+ind] = synth[len(dataset)+ind] + lmbda * grad\n",
    "        self.cnn_dataset = synth\n",
    "        new = synth[n_preds:]\n",
    "        print('Saving new data points')\n",
    "        for ind, new_file in tqdm.tqdm(enumerate(new)):\n",
    "            path = self.dir_path+'/'+str(n_preds+ind)+'.jpg'\n",
    "            self.all_generated_filepaths.append(path)\n",
    "            self.newest_file_paths.append(path)\n",
    "            img = image.array_to_img(new_file)\n",
    "            img.save(path)\n",
    "            \n",
    "    def Ditto_I_Choose_You(self):\n",
    "        print('Calls to API: ',ditto.calls_to_api)\n",
    "        ditto.generate_cnn_dataset()\n",
    "        ditto.instantiate_nn()\n",
    "        while self.calls_to_api+0.5*len(self.cnn_dataset)<self.num_api_calls:\n",
    "            self.Clarifai_query()\n",
    "            self.train_nn()\n",
    "            if self.gens_to_go>0:\n",
    "                self.jacobian_augmentation()\n",
    "    \n",
    "    def YYY(new_datapoint_path):\n",
    "        \"\"\"Outputs variable importance \"\"\"\n",
    "        img = load_img(new_datapoint_path,target_size = (224,224))\n",
    "        img_arr = img_to_array(img)\n",
    "        arr = np.expand_dims(img_arr, axis=0)\n",
    "        arr = preprocess_input(arr)\n",
    "        yhat = self.model.predict(arr)\n",
    "        classification = int(round(yhat[1]))\n",
    "        grad = self.gradients[classification]\n",
    "        jacobed = grad*arr\n",
    "        matrix - k.sum(jacobed)\n",
    "        return sns.heatmap(matrix)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
